{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef44541",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb92eec",
   "metadata": {},
   "source": [
    "Project contains Apache Spark Streaming. Apache Spark write streams to postgre sql, apache hive and deltalake with processing read stream dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d155bc",
   "metadata": {},
   "source": [
    "## Streaming CSV to Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124f6c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python dataframe_to_log.py -idx True -i input/iot_telemetry_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07edd64f",
   "metadata": {},
   "source": [
    "As the above, run python file at console. This provides streaming csv to folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ce88d",
   "metadata": {},
   "source": [
    "## Import Libraries & SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a18f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "import findspark\n",
    "from spark_utils import *\n",
    "\n",
    "findspark.init(\"C:\\Program Files\\Spark\\spark-3.3.1-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60b1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "        .master(\"yarn\")\n",
    "        .appName(\"Spark Streaming with Multiple Sink\")\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0\") \n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "        .config(\"spark.sql.adaptive.enabled\", True)\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad4e3f3",
   "metadata": {},
   "source": [
    "Create spark session with deltalake and apache hive configurations. After that import to delta library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55212d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e6e3e",
   "metadata": {},
   "source": [
    "## IOT Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc93156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iot_schema = \"row_id int, \" \\\n",
    "    \"ts double, \" \\\n",
    "    \"device string, \" \\\n",
    "    \"co float, \" \\\n",
    "    \"humidity float, \" \\\n",
    "    \"light boolean, \" \\\n",
    "    \"lpg float, \" \\\n",
    "    \"motion boolean, \" \\\n",
    "    \"smoke float, \" \\\n",
    "    \"temp float, \" \\\n",
    "    \"time timestamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59796fcd",
   "metadata": {},
   "source": [
    "Make schema for dataframe on the above because spark can read data types wrongly in order that set manually to schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c5f4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"file:///Users/talha/OneDrive/Masaüstü/Talha Nebi Kumru/Data Enginnering/Miuul/RealTimeDPWithSpark/Spark_Streaming_with_Multiple_Sink/output\"\n",
    "delta_path = \"/user/talha/datasets/delta-stream\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea69634",
   "metadata": {},
   "source": [
    "## Read Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f356383",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = (spark.readStream\n",
    "            .format(\"csv\")\n",
    "            .schema(iot_schema)\n",
    "            .option(\"header\", True)\n",
    "            .option(\"maxFilesPerTrigger\", 1)\n",
    "            .load(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c090c38",
   "metadata": {},
   "source": [
    "## Multiple Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83999d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_multiple(df, batchID):\n",
    "    df.persist()\n",
    "    \n",
    "    print(df.limit(5).toPandas().head())\n",
    "    \n",
    "    # Write to PostgreSQL\n",
    "    write_psql(df.filter(\" device == '00:0f:00:70:91:0a' \"), \n",
    "               table_name=\"sensor_0a\", \n",
    "               password=\"password\")\n",
    "    \n",
    "    # Write to Apache Hive\n",
    "    write_hive(df.filter(\" device == 'b8:27:eb:bf:9d:51' \"), \n",
    "               mode=\"append\", \n",
    "               table_name=\"sensor_51\")\n",
    "    \n",
    "    # Write to Deltalake\n",
    "    write_deltalake(df.filter(\" device == '1c:bf:ce:15:ec:4d' \"), \n",
    "                    mode=\"append\", \n",
    "                    path=delta_path)\n",
    "    \n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55050a0c",
   "metadata": {},
   "source": [
    "This function provides appending dataframe to psql, hive and deltalake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534f1d2",
   "metadata": {},
   "source": [
    "## Write Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55899d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0       1  1.594512e+09  ...  19.700001 2023-02-13 01:56:59.308220\n",
      "1       2  1.594512e+09  ...  22.600000 2023-02-13 01:56:59.823688\n",
      "2       3  1.594512e+09  ...  27.000000 2023-02-13 01:57:00.337171\n",
      "3       4  1.594512e+09  ...  22.600000 2023-02-13 01:57:00.851784\n",
      "4       5  1.594512e+09  ...  27.000000 2023-02-13 01:57:01.359949\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      11  1.594512e+09  ...  27.000000 2023-02-13 01:57:04.426730\n",
      "1      12  1.594512e+09  ...  22.600000 2023-02-13 01:57:04.939046\n",
      "2      13  1.594512e+09  ...  27.000000 2023-02-13 01:57:05.440151\n",
      "3      14  1.594512e+09  ...  22.600000 2023-02-13 01:57:05.955877\n",
      "4      15  1.594512e+09  ...  19.700001 2023-02-13 01:57:06.468947\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      21  1.594512e+09  ...  27.000000 2023-02-13 01:57:09.527628\n",
      "1      22  1.594512e+09  ...  22.600000 2023-02-13 01:57:10.028097\n",
      "2      23  1.594512e+09  ...  19.700001 2023-02-13 01:57:10.538998\n",
      "3      24  1.594512e+09  ...  22.600000 2023-02-13 01:57:11.053585\n",
      "4      25  1.594512e+09  ...  19.799999 2023-02-13 01:57:11.567682\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      31  1.594512e+09  ...  22.600000 2023-02-13 01:57:14.615167\n",
      "1      32  1.594512e+09  ...  27.000000 2023-02-13 01:57:15.126961\n",
      "2      33  1.594512e+09  ...  19.799999 2023-02-13 01:57:15.639922\n",
      "3      34  1.594512e+09  ...  22.600000 2023-02-13 01:57:16.155052\n",
      "4      35  1.594512e+09  ...  22.700001 2023-02-13 01:57:16.668001\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      41  1.594512e+09  ...  19.700001 2023-02-13 01:57:19.750844\n",
      "1      42  1.594512e+09  ...  27.000000 2023-02-13 01:57:20.266230\n",
      "2      43  1.594512e+09  ...  22.600000 2023-02-13 01:57:20.779360\n",
      "3      44  1.594512e+09  ...  22.600000 2023-02-13 01:57:21.294096\n",
      "4      45  1.594512e+09  ...  27.000000 2023-02-13 01:57:21.806657\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      51  1.594512e+09  ...  22.600000 2023-02-13 01:57:24.883070\n",
      "1      52  1.594512e+09  ...  27.000000 2023-02-13 01:57:25.397667\n",
      "2      53  1.594512e+09  ...  19.799999 2023-02-13 01:57:25.910995\n",
      "3      54  1.594512e+09  ...  22.600000 2023-02-13 01:57:26.424946\n",
      "4      55  1.594512e+09  ...  27.000000 2023-02-13 01:57:26.937405\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      61  1.594512e+09  ...  27.000000 2023-02-13 01:57:29.993070\n",
      "1      62  1.594512e+09  ...  19.799999 2023-02-13 01:57:30.507226\n",
      "2      63  1.594512e+09  ...  22.700001 2023-02-13 01:57:31.016322\n",
      "3      64  1.594512e+09  ...  22.700001 2023-02-13 01:57:31.528167\n",
      "4      65  1.594512e+09  ...  19.700001 2023-02-13 01:57:32.042472\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      71  1.594512e+09  ...  19.799999 2023-02-13 01:57:35.094386\n",
      "1      72  1.594512e+09  ...  22.600000 2023-02-13 01:57:35.608176\n",
      "2      73  1.594512e+09  ...  27.000000 2023-02-13 01:57:36.118848\n",
      "3      74  1.594512e+09  ...  22.600000 2023-02-13 01:57:36.632284\n",
      "4      75  1.594512e+09  ...  27.000000 2023-02-13 01:57:37.133170\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      81  1.594512e+09  ...  22.600000 2023-02-13 01:57:40.193592\n",
      "1      82  1.594512e+09  ...  19.700001 2023-02-13 01:57:40.706470\n",
      "2      83  1.594512e+09  ...  27.000000 2023-02-13 01:57:41.217995\n",
      "3      84  1.594512e+09  ...  22.600000 2023-02-13 01:57:41.731138\n",
      "4      85  1.594512e+09  ...  22.600000 2023-02-13 01:57:42.246393\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0      91  1.594512e+09  ...  27.000000 2023-02-13 01:57:45.307237\n",
      "1      92  1.594512e+09  ...  19.799999 2023-02-13 01:57:45.820802\n",
      "2      93  1.594512e+09  ...  22.700001 2023-02-13 01:57:46.320929\n",
      "3      94  1.594512e+09  ...  19.700001 2023-02-13 01:57:46.836034\n",
      "4      95  1.594512e+09  ...  22.700001 2023-02-13 01:57:47.348987\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     101  1.594512e+09  ...  19.700001 2023-02-13 01:57:50.416871\n",
      "1     102  1.594512e+09  ...  22.700001 2023-02-13 01:57:50.931802\n",
      "2     103  1.594512e+09  ...  22.600000 2023-02-13 01:57:51.443802\n",
      "3     104  1.594512e+09  ...  27.000000 2023-02-13 01:57:51.944431\n",
      "4     105  1.594512e+09  ...  19.700001 2023-02-13 01:57:52.454405\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     111  1.594512e+09  ...  27.000000 2023-02-13 01:57:55.529672\n",
      "1     112  1.594512e+09  ...  22.600000 2023-02-13 01:57:56.041497\n",
      "2     113  1.594512e+09  ...  19.700001 2023-02-13 01:57:56.554181\n",
      "3     114  1.594512e+09  ...  22.600000 2023-02-13 01:57:57.054437\n",
      "4     115  1.594512e+09  ...  27.000000 2023-02-13 01:57:57.568609\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     121  1.594512e+09  ...  19.799999 2023-02-13 01:58:00.621200\n",
      "1     122  1.594512e+09  ...  27.000000 2023-02-13 01:58:01.132823\n",
      "2     123  1.594512e+09  ...  22.600000 2023-02-13 01:58:01.647252\n",
      "3     124  1.594512e+09  ...  19.700001 2023-02-13 01:58:02.162162\n",
      "4     125  1.594512e+09  ...  22.600000 2023-02-13 01:58:02.671364\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     131  1.594512e+09  ...  22.600000 2023-02-13 01:58:05.738884\n",
      "1     132  1.594512e+09  ...  19.799999 2023-02-13 01:58:06.250179\n",
      "2     133  1.594512e+09  ...  22.600000 2023-02-13 01:58:06.755026\n",
      "3     134  1.594512e+09  ...  19.700001 2023-02-13 01:58:07.267582\n",
      "4     135  1.594512e+09  ...  22.700001 2023-02-13 01:58:07.782948\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     141  1.594512e+09  ...  22.700001 2023-02-13 01:58:10.852953\n",
      "1     142  1.594512e+09  ...  22.700001 2023-02-13 01:58:11.366826\n",
      "2     143  1.594512e+09  ...  19.700001 2023-02-13 01:58:11.879149\n",
      "3     144  1.594512e+09  ...  27.000000 2023-02-13 01:58:12.389185\n",
      "4     145  1.594512e+09  ...  22.700001 2023-02-13 01:58:12.900097\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     151  1.594512e+09  ...  22.700001 2023-02-13 01:58:15.984130\n",
      "1     152  1.594512e+09  ...  19.700001 2023-02-13 01:58:16.494538\n",
      "2     153  1.594512e+09  ...  22.700001 2023-02-13 01:58:16.995272\n",
      "3     154  1.594512e+09  ...  27.000000 2023-02-13 01:58:17.506689\n",
      "4     155  1.594512e+09  ...  19.799999 2023-02-13 01:58:18.017368\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     161  1.594512e+09  ...  19.700001 2023-02-13 01:58:21.085332\n",
      "1     162  1.594512e+09  ...  27.000000 2023-02-13 01:58:21.596877\n",
      "2     163  1.594512e+09  ...  22.600000 2023-02-13 01:58:22.098013\n",
      "3     164  1.594512e+09  ...  19.700001 2023-02-13 01:58:22.608188\n",
      "4     165  1.594512e+09  ...  27.000000 2023-02-13 01:58:23.117833\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     171  1.594512e+09  ...  27.000000 2023-02-13 01:58:26.179605\n",
      "1     172  1.594512e+09  ...  19.799999 2023-02-13 01:58:26.692472\n",
      "2     173  1.594512e+09  ...  22.700001 2023-02-13 01:58:27.193792\n",
      "3     174  1.594512e+09  ...  27.000000 2023-02-13 01:58:27.705817\n",
      "4     175  1.594512e+09  ...  22.700001 2023-02-13 01:58:28.214685\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     181  1.594512e+09  ...  27.000000 2023-02-13 01:58:31.294593\n",
      "1     182  1.594512e+09  ...  19.700001 2023-02-13 01:58:31.807274\n",
      "2     183  1.594512e+09  ...  22.700001 2023-02-13 01:58:32.321794\n",
      "3     184  1.594512e+09  ...  27.100000 2023-02-13 01:58:32.832097\n",
      "4     185  1.594512e+09  ...  22.700001 2023-02-13 01:58:33.343868\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     191  1.594512e+09  ...  27.000000 2023-02-13 01:58:36.398675\n",
      "1     192  1.594512e+09  ...  22.700001 2023-02-13 01:58:36.914082\n",
      "2     193  1.594512e+09  ...  27.100000 2023-02-13 01:58:37.427931\n",
      "3     194  1.594512e+09  ...  22.700001 2023-02-13 01:58:37.942830\n",
      "4     195  1.594512e+09  ...  22.700001 2023-02-13 01:58:38.457325\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     201  1.594512e+09  ...  19.799999 2023-02-13 01:58:41.539502\n",
      "1     202  1.594512e+09  ...  22.700001 2023-02-13 01:58:42.050761\n",
      "2     203  1.594512e+09  ...  19.799999 2023-02-13 01:58:42.564976\n",
      "3     204  1.594512e+09  ...  22.700001 2023-02-13 01:58:43.077864\n",
      "4     205  1.594512e+09  ...  22.700001 2023-02-13 01:58:43.591030\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     211  1.594512e+09  ...  27.000000 2023-02-13 01:58:46.660827\n",
      "1     212  1.594512e+09  ...  22.700001 2023-02-13 01:58:47.173654\n",
      "2     213  1.594512e+09  ...  19.799999 2023-02-13 01:58:47.682781\n",
      "3     214  1.594512e+09  ...  27.000000 2023-02-13 01:58:48.194739\n",
      "4     215  1.594512e+09  ...  22.700001 2023-02-13 01:58:48.705845\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     221  1.594512e+09  ...  22.600000 2023-02-13 01:58:51.788264\n",
      "1     222  1.594512e+09  ...  27.000000 2023-02-13 01:58:52.303511\n",
      "2     223  1.594512e+09  ...  22.600000 2023-02-13 01:58:52.804484\n",
      "3     224  1.594512e+09  ...  19.799999 2023-02-13 01:58:53.319255\n",
      "4     225  1.594512e+09  ...  22.600000 2023-02-13 01:58:53.831008\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     231  1.594512e+09  ...  27.000000 2023-02-13 01:58:56.903922\n",
      "1     232  1.594512e+09  ...  22.600000 2023-02-13 01:58:57.419457\n",
      "2     233  1.594512e+09  ...  19.799999 2023-02-13 01:58:57.955926\n",
      "3     234  1.594512e+09  ...  22.700001 2023-02-13 01:58:58.462332\n",
      "4     235  1.594512e+09  ...  27.000000 2023-02-13 01:58:58.974661\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     241  1.594513e+09  ...  19.799999 2023-02-13 01:59:02.045940\n",
      "1     242  1.594513e+09  ...  22.600000 2023-02-13 01:59:02.558266\n",
      "2     243  1.594513e+09  ...  19.799999 2023-02-13 01:59:03.070681\n",
      "3     244  1.594513e+09  ...  27.000000 2023-02-13 01:59:03.586029\n",
      "4     245  1.594513e+09  ...  22.600000 2023-02-13 01:59:04.095504\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     251  1.594513e+09  ...  27.000000 2023-02-13 01:59:07.171942\n",
      "1     252  1.594513e+09  ...  22.600000 2023-02-13 01:59:07.686019\n",
      "2     253  1.594513e+09  ...  19.700001 2023-02-13 01:59:08.201289\n",
      "3     254  1.594513e+09  ...  22.600000 2023-02-13 01:59:08.713053\n",
      "4     255  1.594513e+09  ...  22.600000 2023-02-13 01:59:09.227627\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     261  1.594513e+09  ...  19.700001 2023-02-13 01:59:12.313067\n",
      "1     262  1.594513e+09  ...  22.600000 2023-02-13 01:59:12.827166\n",
      "2     263  1.594513e+09  ...  27.000000 2023-02-13 01:59:13.337243\n",
      "3     264  1.594513e+09  ...  19.700001 2023-02-13 01:59:13.837252\n",
      "4     265  1.594513e+09  ...  22.600000 2023-02-13 01:59:14.340709\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     271  1.594513e+09  ...  19.700001 2023-02-13 01:59:17.399635\n",
      "1     272  1.594513e+09  ...  22.600000 2023-02-13 01:59:17.900875\n",
      "2     273  1.594513e+09  ...  27.000000 2023-02-13 01:59:18.413638\n",
      "3     274  1.594513e+09  ...  22.600000 2023-02-13 01:59:18.928033\n",
      "4     275  1.594513e+09  ...  19.700001 2023-02-13 01:59:19.446299\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     281  1.594513e+09  ...  22.600000 2023-02-13 01:59:22.531427\n",
      "1     282  1.594513e+09  ...  19.700001 2023-02-13 01:59:23.044938\n",
      "2     283  1.594513e+09  ...  27.000000 2023-02-13 01:59:23.554542\n",
      "3     284  1.594513e+09  ...  22.600000 2023-02-13 01:59:24.064710\n",
      "4     285  1.594513e+09  ...  19.700001 2023-02-13 01:59:24.578465\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     291  1.594513e+09  ...  19.700001 2023-02-13 01:59:27.644425\n",
      "1     292  1.594513e+09  ...  27.000000 2023-02-13 01:59:28.157521\n",
      "2     293  1.594513e+09  ...  22.500000 2023-02-13 01:59:28.672659\n",
      "3     294  1.594513e+09  ...  22.500000 2023-02-13 01:59:29.185144\n",
      "4     295  1.594513e+09  ...  27.000000 2023-02-13 01:59:29.700028\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     301  1.594513e+09  ...  19.700001 2023-02-13 01:59:32.749898\n",
      "1     302  1.594513e+09  ...  22.500000 2023-02-13 01:59:33.262008\n",
      "2     303  1.594513e+09  ...  27.000000 2023-02-13 01:59:33.763576\n",
      "3     304  1.594513e+09  ...  22.500000 2023-02-13 01:59:34.277095\n",
      "4     305  1.594513e+09  ...  27.000000 2023-02-13 01:59:34.791901\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     311  1.594513e+09  ...  19.700001 2023-02-13 01:59:37.869220\n",
      "1     312  1.594513e+09  ...  27.000000 2023-02-13 01:59:38.384578\n",
      "2     313  1.594513e+09  ...  22.400000 2023-02-13 01:59:38.897140\n",
      "3     314  1.594513e+09  ...  19.700001 2023-02-13 01:59:39.398219\n",
      "4     315  1.594513e+09  ...  22.500000 2023-02-13 01:59:39.912922\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     321  1.594513e+09  ...  19.700001 2023-02-13 01:59:42.997403\n",
      "1     322  1.594513e+09  ...  27.000000 2023-02-13 01:59:43.512951\n",
      "2     323  1.594513e+09  ...  22.500000 2023-02-13 01:59:44.013312\n",
      "3     324  1.594513e+09  ...  22.400000 2023-02-13 01:59:44.527875\n",
      "4     325  1.594513e+09  ...  19.700001 2023-02-13 01:59:45.029441\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...       temp                       time\n",
      "0     331  1.594513e+09  ...  27.000000 2023-02-13 01:59:48.094689\n",
      "1     332  1.594513e+09  ...  22.400000 2023-02-13 01:59:48.597005\n",
      "2     333  1.594513e+09  ...  19.700001 2023-02-13 01:59:49.112011\n",
      "3     334  1.594513e+09  ...  22.400000 2023-02-13 01:59:49.625096\n",
      "4     335  1.594513e+09  ...  27.000000 2023-02-13 01:59:50.140641\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id            ts  ...  temp                       time\n",
      "0     341  1.594513e+09  ...  22.4 2023-02-13 01:59:53.193341\n",
      "1     342  1.594513e+09  ...  22.4 2023-02-13 01:59:53.705529\n",
      "2     343  1.594513e+09  ...  27.0 2023-02-13 01:59:54.218743\n",
      "3     344  1.594513e+09  ...  22.4 2023-02-13 01:59:54.730727\n",
      "4     345  1.594513e+09  ...  19.6 2023-02-13 01:59:55.244762\n",
      "\n",
      "[5 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\clientserver.py\", line 467, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\clientserver.py\", line 470, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\clientserver.py\", line 581, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 196, in call\n",
      "    raise e\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 193, in call\n",
      "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
      "  File \"C:\\Users\\talha\\AppData\\Local\\Temp\\ipykernel_18332\\3695326840.py\", line 4, in write_multiple\n",
      "    print(df.limit(5).toPandas().head())\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\", line 157, in toPandas\n",
      "    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 694, in collect\n",
      "    return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 164, in _load_from_socket\n",
      "    sockfile = _create_local_socket(sock_info)\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 139, in _create_local_socket\n",
      "    port = sock_info[0]\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\java_collections.py\", line 201, in __getitem__\n",
      "    return self.__compute_item(key)\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\java_collections.py\", line 177, in __compute_item\n",
      "    new_key = self.__compute_index(key)\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\java_collections.py\", line 166, in __compute_index\n",
      "    size = len(self)\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\java_collections.py\", line 249, in __len__\n",
      "    return get_return_value(answer, self._gateway_client)\n",
      "  File \"C:\\Users\\talha\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.None\n"
     ]
    }
   ],
   "source": [
    "streaming_query = (\n",
    "    stream_df.writeStream\n",
    "        .foreachBatch(write_multiple)\n",
    "        .option(\"checkpointLocation\", \"checkpoint\")\n",
    "        .start()\n",
    ")\n",
    "streaming_query.awaitTermination(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7cf17",
   "metadata": {},
   "source": [
    "## Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d557d",
   "metadata": {},
   "source": [
    "Control to is saved data in psql, hive and deltalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079ff1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "psql_df = read_psql(spark, table_name=\"sensor_0a\", password=\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39576e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>ts</th>\n",
       "      <th>device</th>\n",
       "      <th>co</th>\n",
       "      <th>humidity</th>\n",
       "      <th>light</th>\n",
       "      <th>lpg</th>\n",
       "      <th>motion</th>\n",
       "      <th>smoke</th>\n",
       "      <th>temp</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.594512e+09</td>\n",
       "      <td>00:0f:00:70:91:0a</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>False</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>2023-02-13 01:56:59.308220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1.594512e+09</td>\n",
       "      <td>00:0f:00:70:91:0a</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>False</td>\n",
       "      <td>0.013628</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>2023-02-13 01:57:02.388916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1.594512e+09</td>\n",
       "      <td>00:0f:00:70:91:0a</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>75.800003</td>\n",
       "      <td>False</td>\n",
       "      <td>0.005199</td>\n",
       "      <td>False</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>2023-02-13 01:57:06.468947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>1.594512e+09</td>\n",
       "      <td>00:0f:00:70:91:0a</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>False</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>2023-02-13 01:57:10.538998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>1.594512e+09</td>\n",
       "      <td>00:0f:00:70:91:0a</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>False</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>19.799999</td>\n",
       "      <td>2023-02-13 01:57:11.567682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id            ts  ...       temp                       time\n",
       "0       1  1.594512e+09  ...  19.700001 2023-02-13 01:56:59.308220\n",
       "1       7  1.594512e+09  ...  19.700001 2023-02-13 01:57:02.388916\n",
       "2      15  1.594512e+09  ...  19.700001 2023-02-13 01:57:06.468947\n",
       "3      23  1.594512e+09  ...  19.700001 2023-02-13 01:57:10.538998\n",
       "4      25  1.594512e+09  ...  19.799999 2023-02-13 01:57:11.567682\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psql_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e3596ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_df = read_hive(spark, db_name=\"test\", table_name=\"sensor_51\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91e50ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>ts</th>\n",
       "      <th>device</th>\n",
       "      <th>co</th>\n",
       "      <th>humidity</th>\n",
       "      <th>light</th>\n",
       "      <th>lpg</th>\n",
       "      <th>motion</th>\n",
       "      <th>smoke</th>\n",
       "      <th>temp</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>341</td>\n",
       "      <td>1.594513e+09</td>\n",
       "      <td>b8:27:eb:bf:9d:51</td>\n",
       "      <td>0.004973</td>\n",
       "      <td>49.200001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>2023-02-13 01:59:53.193341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>342</td>\n",
       "      <td>1.594513e+09</td>\n",
       "      <td>b8:27:eb:bf:9d:51</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>49.200001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020512</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>2023-02-13 01:59:53.705529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344</td>\n",
       "      <td>1.594513e+09</td>\n",
       "      <td>b8:27:eb:bf:9d:51</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>49.299999</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020461</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>2023-02-13 01:59:54.730727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>346</td>\n",
       "      <td>1.594513e+09</td>\n",
       "      <td>b8:27:eb:bf:9d:51</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>49.200001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020512</td>\n",
       "      <td>22.299999</td>\n",
       "      <td>2023-02-13 01:59:55.758815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>348</td>\n",
       "      <td>1.594513e+09</td>\n",
       "      <td>b8:27:eb:bf:9d:51</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>49.299999</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020493</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>2023-02-13 01:59:56.782545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id            ts  ...       temp                       time\n",
       "0     341  1.594513e+09  ...  22.400000 2023-02-13 01:59:53.193341\n",
       "1     342  1.594513e+09  ...  22.400000 2023-02-13 01:59:53.705529\n",
       "2     344  1.594513e+09  ...  22.400000 2023-02-13 01:59:54.730727\n",
       "3     346  1.594513e+09  ...  22.299999 2023-02-13 01:59:55.758815\n",
       "4     348  1.594513e+09  ...  22.400000 2023-02-13 01:59:56.782545\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44e29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df = read_delta(spark, delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed5b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>ts</th>\n",
       "      <th>device</th>\n",
       "      <th>co</th>\n",
       "      <th>humidity</th>\n",
       "      <th>light</th>\n",
       "      <th>lpg</th>\n",
       "      <th>motion</th>\n",
       "      <th>smoke</th>\n",
       "      <th>temp</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>343</td>\n",
       "      <td>1.594513e+09</td>\n",
       "      <td>1c:bf:ce:15:ec:4d</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006952</td>\n",
       "      <td>False</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2023-02-13 01:59:54.218743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347</td>\n",
       "      <td>1.594513e+09</td>\n",
       "      <td>1c:bf:ce:15:ec:4d</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>False</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2023-02-13 01:59:56.273850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id            ts  ...  temp                       time\n",
       "0     343  1.594513e+09  ...  27.0 2023-02-13 01:59:54.218743\n",
       "1     347  1.594513e+09  ...  27.0 2023-02-13 01:59:56.273850\n",
       "\n",
       "[2 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81907ab2",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d5ffdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
